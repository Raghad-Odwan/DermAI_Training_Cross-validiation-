# Model Analysis and Training Results — DermAI Training Pipeline

## 1. Overview

This document provides a comprehensive analysis of the training, evaluation, and validation results for the DermAI binary skin lesion classifier (benign vs malignant). The training pipeline implements advanced machine learning practices including stratified cross-validation, ensemble learning, and threshold optimization to ensure robust clinical performance.

**Repository:** https://github.com/Raghad-Odwan/DermAI_Training

### Key Objectives

- Develop a reliable binary classifier for skin lesion diagnosis  
- Ensure generalization through stratified k-fold cross-validation  
- Optimize sensitivity for malignant lesion detection  
- Provide interpretable predictions through Grad-CAM visualization  
- Establish a production-ready model through ensemble techniques  

---

## 2. Dataset and Cross-Validation Strategy

### Dataset Characteristics

- **Total Images:** 19,505 dermoscopic images  
- **Classes:** Binary classification (Benign / Malignant)  
- **Source:** ISIC dataset  
- **Challenge:** Natural class imbalance  

### Stratified 3-Fold Cross-Validation

| Fold | Training Samples | Validation Samples |
|------|------------------|--------------------|
| Fold 1 | ~11,000 | ~5,500–6,500 |
| Fold 2 | ~11,000 | ~5,500–6,500 |
| Fold 3 | ~11,000 | ~5,500–6,500 |

Stratification ensures balanced class distribution and reliable performance estimation.

---

## 3. Model Architecture

### Base Architecture: ResNet50

**Specifications:**
- Backbone: ResNet50 (ImageNet pretrained)
- Trainable Layers: Last 40 layers
- Custom Head:
  - GlobalAveragePooling2D  
  - Dense + Dropout (0.4)  
  - Dense + Dropout (0.3)  
  - Softmax Output  

**Training Configuration:**
- Loss: Binary Cross-Entropy  
- Optimizer: AdamW  
- LR: 1e-5 with ReduceLROnPlateau  
- Regularization: Dropout + Class Weights  
- Early Stopping enabled  

**Architecture Selection Criteria:**
- Best balance of accuracy and efficiency  
- Higher malignant recall  
- Stable cross-fold performance  

---

## 4. Training Pipeline

### Data Augmentation
- Rotation ±40°  
- Width/Height shift ±10%  
- Zoom up to 20%  
- Horizontal flip  
- Brightness ±20%  
- 1/255 pixel scaling  

### Training Parameters
- Batch Size: 32  
- Epochs: 30 per fold  
- Input Size: 224×224  

### Monitoring
- EarlyStopping  
- ReduceLROnPlateau  
- ModelCheckpoint  

---

## 5. Fold-Level Performance Analysis

## Fold-Level Performance Analysis

| Fold | Accuracy | Precision (Malignant) | Recall (Malignant) | F1-Score |
|------|----------|-----------------------|--------------------|----------|
| Fold 1 | 0.8368 | 0.7780 | 0.6821 | 0.7269 |
| Fold 2 | 0.8288 | 0.7415 | 0.7097 | 0.7253 |
| Fold 3 | 0.7983 | 0.6504 | 0.7928 | 0.7146 |


### Cross-Fold Summary

| Metric | Mean | Range |
|--------|------|--------|
| Accuracy | 0.822 | 0.798–0.837 |
| Malignant Recall | 0.727 | 0.682–0.793 |
| Malignant F1 | 0.722 | 0.715–0.727 |
| Benign Recall | 0.890 | — |

---

## 6. Ensemble Model Analysis

### Method
Ensemble = Average probabilistic outputs of the 3 folds.

### Benefits
- Reduced variance  
- Better calibration  
- Improved robustness  
- Smoother probability distribution  

### Ensemble Performance

| Metric | Value |
|--------|--------|
| Accuracy | 0.830 |
| Malignant Recall | 0.730 |
| Stability | High |

---

## 7. Threshold Optimization

### Evaluation
Default threshold (0.5) is conservative.

### Threshold Comparison

| Threshold | Recall | Precision | F1 | Suitability |
|-----------|---------|-----------|--------|-------------|
| 0.35 | High | Moderate | Balanced | High sensitivity |
| 0.40 | Optimal | Good | Best | Recommended |
| 0.45 | Good | Better | Good | Balanced |
| 0.50 | Moderate | Best | Moderate | Conservative |

**Recommended Threshold:** 0.40

---

## 8. Strengths of the Training Pipeline

### Technical
- Reproducible  
- Robust multi-fold consistency  
- Efficient training workflow  
- Modular architecture  

### Methodological
- Stratified k-fold cross-validation  
- Ensemble learning  
- Grad-CAM explainability  
- Threshold tuning  
- Complete evaluation beyond accuracy  

### Documentation
- Full logs  
- Complete metrics  
- ROC/Confusion Plots  
- Clear code separation  

---

## 9. Limitations and Future Improvements

### Limitations
- Malignant recall ~72–73% (below ideal 75–80%)  
- Class imbalance persists  
- Limited rare subtype coverage  
- Low input resolution  

### Proposed Improvements
**Short-Term**
- Increase trainable layers  
- Higher resolution input  
- Stronger augmentation  

**Medium-Term**
- Focal Loss  
- Attention layers  
- EfficientNetV2 / ConvNeXt  

**Long-Term**
- Multi-class support  
- Metadata integration  
- Mobile optimization  
- Continuous learning pipeline  

---

## 10. Next Steps

### Full Dataset Training for Production

**Plan:**
1. Use Fold 1 weights as initialization  
2. Train on full 19,505 images  
3. Apply optimal hyperparameters  
4. Enhanced augmentations  
5. Validate on independent test set  

**Expected Outcomes:**
- Higher malignant sensitivity  
- Improved generalization  
- Final deployable model  

---

## 11. Comparative Context

### Research Alignment
- Literature ResNet50 baselines: Accuracy 0.80–0.85, F1 0.70–0.75  
- DermAI: Accuracy 0.83, F1 0.72  

### Academic Contributions
- Rigorous cross-validation  
- Ensemble methodology  
- Explainability  
- Threshold optimization  
- Fully reproducible  

---

## 12. Conclusion

The DermAI training pipeline provides a robust, clinically-oriented model development strategy. Through cross-validation, ensemble learning, and threshold analysis, the system prioritizes malignant detection while maintaining stability and interpretability.

### Key Achievements
- Consistent cross-fold results  
- Ensemble stability  
- Grad-CAM support  
- Threshold optimization  
- Production-ready architecture  

The pipeline demonstrates research-grade quality and strong potential for deployment in clinical decision support systems.

---

## 13. Technical Specifications Summary

### Environment
- Framework: TensorFlow/Keras  
- Hardware: GPU  
- Python: 3.8+  

### Model Files
- best_resnet50_fold1.keras  
- best_resnet50_fold2.keras  
- best_resnet50_fold3.keras  
- Training Logs  
- Metrics JSON Files  

### Generated Artifacts
- ROC curves  
- Confusion matrices  
- Loss/Accuracy curves  
- Grad-CAM samples  
- Threshold analysis reports  

---

## References

Repository:  
https://github.com/Raghad-Odwan/DermAI_Training

Related Components:  
- Image Validation Module  
- GradCAM Module  
- Comparative Algorithms Study  
- Final Training Pipeline  
